#!/usr/bin/perl -w
#
# analyze-pages - parse the output of /dev/pageinfo
#

use strict;
use List::Util qw/reduce/;

#
# TODO: Need to support superpages
#
my $PAGE_SIZE = 4096;

#
# Open the right file - either given on the command line or stdin
#
if (@ARGV) {
    if (@ARGV == 1) {
        open FH, "<$ARGV[0]";
    } else {
        die "usage: $0 [filename]\n";
    }
} else {
    # Read from stdin
    open FH,"-";
}

#
# Bookkeeping structures
#
my %hashvals;
my %hotpages;
my %reservedpages;
my %reclaimpages;
my $total_pages;
my $allocated_pages;
#
# Look through all pages
#
while (<FH>) {
    # Note: This line will need to change whenever we change the output format
    if (/^\s*(\d+): (0x[0-9a-f]+), ([0-9a-f]+), (\d+), (\d), (\d), (\d)$/) {
        my ($page,$address,$hash,$users,$hot, $reserved, $reclaim) 
			= ($1,$2,$3,$4,$5,$6,$7);

        $total_pages++;

        #
        # We only count most statistics for pages that are in-use, so skip out
        # early if it's free
        #
        if ($users < 1) {
            next;
        }

        $allocated_pages++;

        #
        # Build up a perl hash of the hash values seen for pages
        #
        if (!exists($hashvals{$hash})) {
            $hashvals{$hash} = 1;

			# hot pages
            if ($hot) {
                $hotpages{$hash} = 1;
            } else {
                $hotpages{$hash} = 0;
            }

			# reserved pages
            if ($reserved) {
                $reservedpages{$hash} = 1;
            } else {
                $reservedpages{$hash} = 0;
            }

			# reclaimable pages
            if ($reclaim) {
                $reclaimpages{$hash} = 1;
            } else {
                $reclaimpages{$hash} = 0;
            }

        } else {
            $hashvals{$hash}++;
            if ($hot) { $hotpages{$hash}++; }
			if ($reserved) { $reservedpages{$hash}++; }
			if ($reclaim) { $reclaimpages{$hash}++; }
        }

    } else {
        warn "Unsupported line\n";
    }
}

#
# Report statistics
#
print "Total pages: $total_pages (" . ($total_pages * $PAGE_SIZE / 1024) .
      " kB)\n";
printf "Allocated pages: $allocated_pages (%.02f%%)\n",
    ($allocated_pages * 100.0 / $total_pages);

printf "Unique hashes: " . scalar(keys(%hashvals)) .
    " (%.02f%% of allocated pages)\n",
    (scalar(keys(%hashvals)) * 100.0 / $allocated_pages);

# List of duplicated hashes    
my @duphashes = grep {$hashvals{$_} > 1} keys(%hashvals);

# Sum of all pages that belong to a duplicated hash group
my $duppages = reduce(sub {$a + $b} , 0,@hashvals{@duphashes});

# Maximum number of pages we could free
my $freeablepages = reduce(sub {$a + ($b-1)} , 0,@hashvals{@duphashes});

# Get number of the most duplicated pages. In most cases, it is the zeroed page.
my ($largest_key, $largest_val) = each %hashvals;
while (my($key, $val) = each %hashvals) {
	if($val > $largest_val) {
		$largest_key = $key;
		$largest_val = $val;
	}
}

printf "Duplicated pages: " . $duppages .
    " (%.02f%% of allocated pages)\n",
    ($duppages * 100.0 / $allocated_pages);

printf "Freeable pages: " . $freeablepages .
    " (%.02f%% of allocated pages)\n",
    ($freeablepages * 100.0 / $allocated_pages);

printf "Freeable and nonzeroed pages: %d " . 
    " (%.02f%% of allocated pages)\n",
	$freeablepages - $largest_val,
    (($freeablepages-$largest_val) * 100.0 / $allocated_pages);
	

my $totalhot = reduce( sub {$a + $b}, 0, values(%hotpages) );
printf "Total hot pages: $totalhot (%.02f%% of allocated pages)\n",
    ($totalhot * 100.0 / $allocated_pages);

my $hotdup = reduce( sub {$a + $b}, 0, @hotpages{@duphashes} );
printf "Duplicated hot pages: $hotdup (%.02f%% of dup. pages)\n",
    ($hotdup * 100.0 / $duppages);

print "The top 20 common hash values:\n";
print "hash:\t\t\t\t\t\ttotal:reserved:nonreser:reclaim:\n";
foreach my $hash ((sort {$hashvals{$b} <=> $hashvals{$a}}
        keys %hashvals)[0..19]) {
    printf "$hash\t$hashvals{$hash}\t$reservedpages{$hash}\t".
		   "%d\t$reclaimpages{$hash}\n", $hashvals{$hash}-$reservedpages{$hash};
}
